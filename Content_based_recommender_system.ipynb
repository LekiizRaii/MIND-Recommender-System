{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. First steps"
      ],
      "metadata": {
        "id": "lk_Is9zWmNeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Install PySpark for the project"
      ],
      "metadata": {
        "id": "97hPm98fnhu6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOciDEz7ovgw",
        "outputId": "ca672f90-09a7-4ca5-d723-5fe59d2b40f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=6d8de0bcecad73c2ed1ed6c9685dd6d53a59ecd0dc1ca7564a7412e719843b1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --force-reinstall pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Connect my drive for MINDsmall dataset access"
      ],
      "metadata": {
        "id": "GIBz3B1_mVcA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt2SV4Olr04s",
        "outputId": "00bcade5-5bf1-44b1-8761-db7b9dcb5c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "dir = 'drive/MyDrive/Datasets'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Import all necessary utilities and create Spark session"
      ],
      "metadata": {
        "id": "kx-tra_kmnbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-CscjIBo7aS"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import datetime as dt\n",
        "# from functools import reduce\n",
        "# from operator import add\n",
        "# Import all Spark utilities for data processing\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DateType, TimestampType, StructType, StructField, IntegerType, StringType, FloatType, DoubleType, ArrayType\n",
        "from pyspark.sql.functions import col, sqrt, desc, asc, split, explode, from_json, get_json_object, inline\n",
        "from pyspark.sql.functions import from_unixtime, unix_timestamp, array, monotonically_increasing_id, lit, min, max, to_date\n",
        "\n",
        "# Import all Spark utilities for MLP architecture\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.executor.memory\", \"12g\") \\\n",
        "    .config(\"spark.driver.memory\", \"12g\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\", True) \\\n",
        "    .config('spark.sql.parquet.int96RebaseModeInRead', 'LEGACY') \\\n",
        "    .config('spark.sql.parquet.int96RebaseModeInWrite', 'LEGACY') \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"12g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\",64) \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data processing"
      ],
      "metadata": {
        "id": "6V6BKq7InZ48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Combine data from train and dev files"
      ],
      "metadata": {
        "id": "XpBTz1oGn30f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFjbpimAhcqR"
      },
      "outputs": [],
      "source": [
        "# Policy for time converting\n",
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
        "\n",
        "# Load behaviors.tsv from train dataset\n",
        "behaviors_train_df = spark.read.csv(dir + '/MINDsmall/MINDsmall_train/behaviors.tsv', sep=r'\\t', header=False) \\\n",
        "    .selectExpr('_c0 AS impressionID',\n",
        "                '_c1 AS userID',\n",
        "                '_c2 AS time',\n",
        "                '_c3 AS history',\n",
        "                '_c4 AS impressions')\n",
        "\n",
        "# Load news.tsv from train dataset\n",
        "news_train_df = spark.read.csv(dir + '/MINDsmall/MINDsmall_train/news.tsv', sep=r'\\t', header=False) \\\n",
        "    .selectExpr('_c0 AS newsID',\n",
        "                '_c1 AS category',\n",
        "                '_c2 AS subcategory',\n",
        "                '_c3 AS title',\n",
        "                '_c4 AS abstract',\n",
        "                '_c5 AS url',\n",
        "                '_c6 AS titleEntities',\n",
        "                '_c7 AS abstractEntities')\n",
        "\n",
        "# Load behaviors.tsv from dev dataset\n",
        "behaviors_dev_df = spark.read.csv(dir + '/MINDsmall/MINDsmall_dev/behaviors.tsv', sep=r'\\t', header=False) \\\n",
        "    .selectExpr('_c0 AS impressionID',\n",
        "                '_c1 AS userID',\n",
        "                '_c2 AS time',\n",
        "                '_c3 AS history',\n",
        "                '_c4 AS impressions')\n",
        "\n",
        "# Load news.tsv from dev dataset\n",
        "news_dev_df = spark.read.csv(dir + '/MINDsmall/MINDsmall_dev/news.tsv', sep=r'\\t', header=False) \\\n",
        "    .selectExpr('_c0 AS newsID',\n",
        "                '_c1 AS category',\n",
        "                '_c2 AS subcategory',\n",
        "                '_c3 AS title',\n",
        "                '_c4 AS abstract',\n",
        "                '_c5 AS url',\n",
        "                '_c6 AS titleEntities',\n",
        "                '_c7 AS abstractEntities')\n",
        "\n",
        "# Combine data from train and dev datasets\n",
        "behaviors_df = behaviors_train_df.union(behaviors_dev_df).dropDuplicates()\n",
        "behaviors_df = behaviors_df.withColumn(\"time\", to_date(col(\"time\"), \"MM/dd/yyyy\"))\n",
        "news_df = news_train_df.union(news_dev_df).dropDuplicates()\n",
        "\n",
        "# behaviors_df.show(n=5)\n",
        "# print(\"behaviors_df's rows: \", behaviors_df.count())\n",
        "\n",
        "# news_df.show(n=5)\n",
        "# print(\"news_df's rows: \", news_df.count())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"impressionID NULL value count: \", behaviors_df.where(col(\"impressionID\").isNull()).count())\n",
        "print(\"userID NULL value count: \", behaviors_df.where(col(\"userID\").isNull()).count())\n",
        "print(\"time NULL value count: \", behaviors_df.where(col(\"time\").isNull()).count())\n",
        "print(\"history NULL value count: \", behaviors_df.where(col(\"history\").isNull()).count())\n",
        "print(\"impressions NULL value count: \", behaviors_df.where(col(\"impressions\").isNull()).count())\n",
        "\n",
        "# Show samples of NULL value history\n",
        "# behaviors_df.where(col(\"history\").isNull()).show(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVjQAUQL4oVk",
        "outputId": "f9f3dbf4-35ba-462a-c4af-69b5a0fed922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "impressionID NULL value count:  0\n",
            "userID NULL value count:  0\n",
            "time NULL value count:  0\n",
            "history NULL value count:  5452\n",
            "impressions NULL value count:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"newsID NULL value count: \", news_df.where(col(\"newsID\").isNull()).count())\n",
        "print(\"category NULL value count: \", news_df.where(col(\"category\").isNull()).count())\n",
        "print(\"subcategory NULL value count: \", news_df.where(col(\"subcategory\").isNull()).count())\n",
        "print(\"title NULL value count: \", news_df.where(col(\"title\").isNull()).count())\n",
        "print(\"abstract NULL value count: \", news_df.where(col(\"abstract\").isNull()).count())\n",
        "print(\"url NULL value count: \", news_df.where(col(\"url\").isNull()).count())\n",
        "print(\"titleEntities NULL value count: \", news_df.where(col(\"titleEntities\").isNull()).count())\n",
        "print(\"abstractEntities NULL value count: \", news_df.where(col(\"abstractEntities\").isNull()).count())\n",
        "\n",
        "# Show dataframes of NULL-value-having columns\n",
        "# news_df.where(col(\"abstract\").isNull()).show(n=5)\n",
        "# news_df.where(col(\"titleEntities\").isNull()).show(n=5)\n",
        "# news_df.where(col(\"abstractEntities\").isNull()).show(n=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCsTlm0R6iT5",
        "outputId": "030520a3-4a1a-4f78-a106-f967fb1e9678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newsID NULL value count:  0\n",
            "category NULL value count:  0\n",
            "subcategory NULL value count:  0\n",
            "title NULL value count:  0\n",
            "abstract NULL value count:  3415\n",
            "url NULL value count:  0\n",
            "titleEntities NULL value count:  3\n",
            "abstractEntities NULL value count:  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAOrs9AvuLch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25cd3ec-6d4a-4c46-d97b-65ff47228376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|userID|userCount|\n",
            "+------+---------+\n",
            "|U32146|       69|\n",
            "|U44201|       50|\n",
            "|U57047|       45|\n",
            "|U15740|       45|\n",
            "|U20833|       41|\n",
            "+------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+---------+\n",
            "|newsID|newsCount|\n",
            "+------+---------+\n",
            "|N48865|        1|\n",
            "|N56379|        1|\n",
            "|N56442|        1|\n",
            "|N11904|        1|\n",
            "|N48051|        1|\n",
            "+------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_count_df = behaviors_df.select('userID').groupBy('userID').count().withColumnRenamed(\"count\",\"userCount\").orderBy(desc('userCount'))\n",
        "news_count_df = news_df.select('newsID').groupBy('newsID').count().withColumnRenamed(\"count\",\"newsCount\").orderBy(desc('newsCount'))\n",
        "\n",
        "user_count_df.show(n=5)\n",
        "news_count_df.show(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Create table of users' click activities"
      ],
      "metadata": {
        "id": "ny5BgtHFo9ZS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiJB6sNhESZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab2a47d-2fb8-4aaf-ef42-b18aeb1970c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----+----------+\n",
            "|userID|newsID|click|      time|\n",
            "+------+------+-----+----------+\n",
            "|    U1|N10646|  1.0|2019-11-15|\n",
            "|    U1|N13374|  1.0|2019-11-15|\n",
            "|    U1|N14637|  0.0|2019-11-15|\n",
            "|    U1|N62058|  1.0|2019-11-15|\n",
            "|   U10|N11784|  0.0|2019-11-15|\n",
            "+------+------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Total interacts:  9440772\n",
            "Total clicked interacts:  2356550\n"
          ]
        }
      ],
      "source": [
        "# Load history click activities\n",
        "history_click_df = behaviors_df.select(\"userID\", \"time\",\n",
        "                                       explode(split(\"history\", \" \").alias(\"newsID\")).alias(\"newsID\")) \\\n",
        "                                       .withColumn(\"click\", lit(1.0).cast(DoubleType()))\n",
        "\n",
        "# Load current click activities\n",
        "current_click_df = behaviors_df.select(\"userID\", \"time\",\n",
        "                                    explode(split(\"impressions\", \" \").alias(\"click\")).alias(\"click\"))\n",
        "current_click_df = current_click_df.select(\"userID\", \"time\",\n",
        "                                     split(\"click\", \"-\").alias(\"click\"))\n",
        "current_click_df = current_click_df.select(\"userID\", \"time\",\n",
        "                                           current_click_df.click.getItem(0).alias(\"newsID\"),\n",
        "                                           current_click_df.click.getItem(1).alias(\"click\").cast(DoubleType()))\n",
        "\n",
        "# Combine data from history and current click activities\n",
        "user_click_df = history_click_df.union(current_click_df).dropDuplicates()\n",
        "user_click_df = user_click_df.groupBy(\"userID\", \"newsID\", \"click\").agg(min(\"time\").alias(\"time\"))\n",
        "temp_df = user_click_df.groupBy(\"userID\", \"newsID\").agg(max(\"click\").alias(\"click\"))\n",
        "user_click_df = user_click_df.join(temp_df, on=[\"userID\", \"newsID\", \"click\"], how=\"inner\")\n",
        "\n",
        "\n",
        "user_click_df.show(n=5)\n",
        "print(\"Total interacts: \", user_click_df.count())\n",
        "print(\"Total clicked interacts: \", user_click_df.where(\"click==1.0\").count())\n",
        "# print(user_click_df.count())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"userID NULL value count: \", user_click_df.where(col(\"userID\").isNull()).count())\n",
        "print(\"newsID NULL value count: \", user_click_df.where(col(\"newsID\").isNull()).count())\n",
        "print(\"click NULL value count: \", user_click_df.where(col(\"click\").isNull()).count())\n",
        "print(\"time NULL value count: \", user_click_df.where(col(\"time\").isNull()).count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7SgRnNo8Uzk",
        "outputId": "ecb34a28-cbe8-4399-d5af-9b544b8634d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "userID NULL value count:  0\n",
            "newsID NULL value count:  0\n",
            "click NULL value count:  0\n",
            "time NULL value count:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dDGIFf-P1zn",
        "outputId": "68fc1993-4b62-4962-a30e-83e7ec6c0d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+---------------+\n",
            "|userID|clickCount|totalAppearance|\n",
            "+------+----------+---------------+\n",
            "|U63482|       658|           1445|\n",
            "|U59594|       504|            982|\n",
            "|U84756|       499|           1261|\n",
            "| U2784|       467|            956|\n",
            "|U72489|       463|           1064|\n",
            "+------+----------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "temp_df_1 = user_click_df.where(\"click==1.0\").groupBy(\"userID\").count().withColumnRenamed(\"count\",\"clickCount\").orderBy(desc(\"clickCount\"))\n",
        "temp_df_2 = user_click_df.groupBy(\"userID\").count().withColumnRenamed(\"count\",\"totalAppearance\").orderBy(desc(\"totalAppearance\"))\n",
        "temp_df_3 = temp_df_1.join(temp_df_2, on=[\"userID\"], how=\"left\").orderBy(desc(\"clickCount\"))\n",
        "\n",
        "temp_df_3.show(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Create embedding news PySpark Dataframe from calculated dataframes"
      ],
      "metadata": {
        "id": "ZIAFyPTvpVhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfmF-OvSafi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb3d017-6713-4309-a4d8-53d2285f9c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+\n",
            "|entityID|        1|        2|        3|        4|        5|        6|        7|        8|        9|      10|       11|       12|       13|       14|       15|       16|       17|       18|       19|       20|       21|       22|       23|       24|       25|       26|       27|       28|       29|      30|       31|       32|      33|       34|       35|       36|       37|       38|       39|       40|       41|      42|       43|       44|       45|       46|       47|       48|       49|       50|       51|       52|       53|       54|       55|       56|       57|       58|      59|       60|       61|       62|       63|       64|       65|       66|       67|       68|       69|       70|       71|       72|      73|      74|       75|       76|       77|       78|      79|       80|       81|       82|       83|       84|       85|       86|       87|      88|       89|       90|       91|       92|       93|       94|       95|       96|      97|       98|       99|      100|\n",
            "+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+\n",
            "|     Q33|-0.080682|-0.145228|   0.0668|-0.029954| 0.018691| -0.15177| -0.07849|-0.161835|-0.002182|0.060755| 0.019456| 0.032687| 0.024165|-0.066258|-0.031357|-0.075461|-0.048182| 0.057226|   0.1001|-0.172107| 0.047223|-0.119463| -0.14388| 0.275786| 0.085236|-0.023965| 0.053698|-0.082129| 0.116759|0.104196|-0.104459| 0.063871|0.113942|-0.038695|-0.002741| 0.056361| 0.022789|-0.119106|-0.152339| 0.092205|-0.040168|0.024594| 0.057784| 0.052263|-0.148822| 0.086025|-0.097875| 0.149933| 0.061954|-0.034819| -0.23319|-0.045487|-0.089148| 0.019617| 0.026751| 0.023336|-0.057799| 0.059624|0.072403|-0.162809|-0.028255| 0.149708| 0.020321|  0.11908|-0.122214|-0.066723|-0.154117|-0.152033|-0.129829|-0.120924| 0.037756|-0.089105|0.050801|0.056504|-0.121939|-0.079465|-0.025157| 0.185286|0.072012|  0.08863| 0.057734|-0.136504| 0.176968| 0.088408|-0.011963|-0.064144|-0.047188|0.076381|  0.01731|-0.162435|-0.049524|  0.01307| 0.064148|-0.113552| 0.009358|-0.258418|0.095019|-0.001529|-0.092155| 0.047749|\n",
            "|    Q641| 0.053446|-0.112674| 0.122613| 0.021186| 0.018508|-0.084293| 0.031426|-0.053035| 0.018017|0.087175| 0.041003| 0.037649| 0.080479|-0.112106|-0.152385| -0.01616|-0.098292| 0.033057| 0.072083| 0.009638|-0.038584| 0.137189|-0.038173| 0.067215|-0.068079| 0.038179|-0.092389|-0.126342| 0.005221|0.030714|-0.003846|-0.065576|0.030757| 0.032275|-0.127036| 0.174622|-0.080785|-0.073039|-0.152005|-0.002524|-0.060511|0.026828|-0.011424|-0.094375| -0.18253|-0.060045| 0.038562| 0.050718|-0.005907| 0.116229|-0.150546| 0.020803| 0.061855|-0.052031|  1.99E-4|-0.026402|-0.008029|-0.005046|0.081349|-0.160672|-0.071677| 0.002949| 0.024049| 0.055977|-0.026948|-0.060171|-0.181429|-0.074527|-0.097864|-0.140592|-0.073337|-0.080326|0.077498|0.165838|-0.065002|-0.024691| -0.04004| 0.054038|0.084707|-0.057484| 0.049312|-0.022656|-0.020299| 0.001749| 0.138085|-0.069931| 0.005662|0.149884|-0.078353| 0.005409|-0.070555|-0.085013|-0.057181|-0.146676|-0.127118|-0.096958|0.137803|-0.097044|-0.105607| 0.052735|\n",
            "|     Q16| 0.004779|-0.015944|-0.007751|-0.063069|-0.021705|-0.082206|-0.115454|-0.102872| 0.134496|0.017096| 0.006877|-0.010387|-0.011354|-0.035858|-0.042879| 0.049632| 0.047991| 0.060972|-0.067812|-0.170605|-0.101364| 0.041804|-0.160965| 0.120239| 0.093469| -0.09796| 0.059874| 0.017113| 0.023784| 0.00583|-0.124724| 0.008893|0.079826|-0.040756| 0.045164| 0.019453|-0.107043|-0.190856|-0.156236|-0.009638| 0.085847|0.082165|-0.128597|-0.046085|-0.206859| 0.045966|-0.112015| 0.146121| -0.05887| 0.103533|-0.084829|-0.005227| 0.086092| 0.092352|-0.062433| 0.099007| 0.065441| 0.086386|0.021127|-0.032853|-0.053546| 0.062233| 0.124049|  0.01219|-0.107048| -0.08854|-0.079479|-0.034667|-0.072911|-0.126074| -0.14688|-0.102316|0.097981|0.098867|-0.113757|-0.152845|-0.070388| 0.223492|0.101513| 0.104134|-0.060284| 0.011534|-0.032341| 0.057851|  0.06354|  -0.1462| 0.008716|0.207419|-0.160273|-0.135902|-0.012419| 0.102466|  0.05634|-0.138549|-0.228548|-0.144285|0.075921|-0.016819|-0.076273| 0.212443|\n",
            "|  Q40348|-0.104565|-0.069042|-0.117033|-0.045549| 0.092503|-0.215117|-0.080553|-0.012469| 0.112993|0.021602| 0.156261|-0.028978| 0.028104|-0.067826|-0.031297| 0.113351|-0.031901|-0.030473|-0.194254|-0.005225|-0.119527| 0.082012|  -0.0076| 0.031394|-0.167783| 0.236334|-0.001901| 0.006504|-0.143117|0.025784| 0.112939| 0.185826|0.007873|-0.180003| 0.152178|-0.065803| 0.033227|-0.047413| 0.097256|-0.153422| 0.134994|0.007143| 0.045777|-0.064075| 0.057293|   0.1706| 0.037305|-0.023057|-0.025279|-0.049761|-0.034535|-0.076106| 0.110585| 0.056011| 0.178342|-0.200286|-0.059178|-0.051372|0.071344|-0.108768|-0.099522|-0.029437| 0.122276|-0.058178| -0.04689| 0.140283|-0.132648| 0.117795| 0.130444|-0.127342| 0.183794|-0.070907|0.168586|0.049002|  0.10565|-0.026223|-0.089938|-0.055604|0.025271| 0.010056|-0.031055| -0.15363|  0.01991|-0.042508| 0.120997|-0.121802|-0.071877|0.093137|-0.102055| 0.128244| -0.16989|-0.005506| 0.040637|-0.063967| 0.014684|-0.060801|   0.031| 0.030388|-0.056875| 0.098278|\n",
            "|    Q571| 0.025624|-0.034782|-0.004034|-0.084482|-0.003195|-0.115983| -0.10748|-0.011599| 0.096178|0.056604|-0.092722| 0.089413|-0.077867|-0.081205|-0.057782| 0.072082|-0.065045|-0.141947|-0.038944| 0.067072| 0.002128|-0.024427| -0.09969|-0.002762| 0.029647|-0.027511|-0.103125|-0.132785| 0.082134|0.009824|-0.046993| 0.091732|0.048746| 0.005484| 0.122875|-0.061967| -0.10597|-0.081322|-0.072035|-0.095984| 0.210199|0.048357| 0.061702| 0.007493|-0.077751| 0.124151| 0.084196|-0.081242| 0.093894| 0.049973|-0.077089| 0.092274|-0.012638|-0.012789| 0.026172|-0.112679|-0.096477| 0.057355|  0.0906|-0.159996|-0.119803| 0.061503|-0.080908|-0.069363| 0.040846| 0.119286|-0.073668| 0.087687|-0.027143|-0.066039| 0.138385|-0.114736|0.140309|0.048047| 0.012171| 0.071823| 0.004272|-0.014949|0.072866| 0.001452|-0.002894|-0.022664|-0.130075| 0.074978|-0.123239|-0.052696|-0.102723|0.055303| -0.10161| 0.097411| 0.021788|-0.098112|-0.011269|-0.134358|-0.120166|-0.082104|0.045755| 0.013568| 0.125909|-0.030209|\n",
            "+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------+--------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+---------+---------+---------+---------+---------+--------+---------+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load embedding vectors file\n",
        "embedding_train_df = spark.read.csv(dir + '/MINDsmall/MINDsmall_train/entity_embedding.csv', header=True) \\\n",
        "    .withColumnRenamed(\"0\",\"entityID\")\n",
        "\n",
        "embedding_dev_df = spark.read.csv(dir + '/MINDsmall/MINDsmall_dev/entity_embedding.csv', header=True) \\\n",
        "    .withColumnRenamed(\"0\",\"entityID\")\n",
        "\n",
        "embedding_df = embedding_train_df.union(embedding_dev_df).dropDuplicates()\n",
        "embedding_df = embedding_df.select(\"entityID\", *(col(c).cast(DoubleType()).alias(c) for c in embedding_df.columns[1:]))\n",
        "\n",
        "embedding_df.show(n=5)\n",
        "# print(embedding_df.schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgqJOLq5dWyR"
      },
      "outputs": [],
      "source": [
        "titleSchema = ArrayType(\n",
        "    StructType([\n",
        "        StructField(\"Label\", StringType(), True),\n",
        "        StructField(\"Type\", StringType(), True),\n",
        "        StructField(\"WikidataId\", StringType(), True),\n",
        "        StructField(\"Confidence\", FloatType(), True),\n",
        "        StructField(\"OccurrenceOffsets\", StringType(), True),\n",
        "        StructField(\"SurfaceForms\", ArrayType(StringType()), True)\n",
        "    ])\n",
        ")\n",
        "\n",
        "abstractSchema = ArrayType(\n",
        "    StructType([\n",
        "        StructField(\"Label\", StringType(), True),\n",
        "        StructField(\"Type\", StringType(), True),\n",
        "        StructField(\"WikidataId\", StringType(), True),\n",
        "        StructField(\"Confidence\", FloatType(), True),\n",
        "        StructField(\"OccurrenceOffsets\", StringType(), True),\n",
        "        StructField(\"SurfaceForms\", ArrayType(StringType()), True)\n",
        "    ])\n",
        ")\n",
        "\n",
        "\n",
        "# Create news_embedding_df PySpark DataFrame\n",
        "news_title_embedding_df = news_df.withColumn('newTitleEntities', from_json(\"titleEntities\", titleSchema))\n",
        "news_abstract_embedding_df = news_df.withColumn('newAbstractEntities', from_json(\"abstractEntities\", abstractSchema))\n",
        "\n",
        "news_title_embedding_df = news_title_embedding_df.selectExpr(\"inline(newTitleEntities)\", \"newsID\")\n",
        "news_title_embedding_df = news_title_embedding_df.select(\"newsID\", \"WikidataId\")\n",
        "\n",
        "news_abstract_embedding_df = news_abstract_embedding_df.selectExpr(\"inline(newAbstractEntities)\", \"newsID\")\n",
        "news_abstract_embedding_df = news_abstract_embedding_df.select(\"newsID\", \"WikidataId\")\n",
        "\n",
        "news_embedding_df = news_title_embedding_df.union(news_abstract_embedding_df).dropDuplicates()\n",
        "news_embedding_df = news_embedding_df.select(\"newsID\", col(\"WikidataId\").alias(\"entityID\"))\n",
        "\n",
        "\n",
        "# Join the embedded vector with the news_embedding_df\n",
        "news_embedding_df = news_embedding_df.join(embedding_df, on=[\"entityID\"], how=\"inner\")\n",
        "news_embedding_df = news_embedding_df.select(*(col(x) for x in news_embedding_df.columns[1:]))\n",
        "new_columns = news_embedding_df.columns[1:]\n",
        "\n",
        "\n",
        "# Sum up all the vectors of one news\n",
        "news_embedding_df = news_embedding_df.groupBy(\"newsID\").sum()\n",
        "old_columns = news_embedding_df.columns[1:]\n",
        "\n",
        "\n",
        "for idx in range(len(old_columns)):\n",
        "  news_embedding_df = news_embedding_df.withColumnRenamed(old_columns[idx], new_columns[idx])\n",
        "\n",
        "\n",
        "# Normalize the embedded vector for the news\n",
        "# news_embedding_df = news_embedding_df.withColumn(\"normalization\", sqrt(reduce(add, [col(x) * col(x) for x in news_embedding_df.columns[1:]])))\n",
        "news_embedding_df = news_embedding_df.withColumn(\"normalization\", sqrt(sum(col(x) * col(x) for x in news_embedding_df.columns[1:])))\n",
        "\n",
        "new_columns = news_embedding_df.columns[1:-1]\n",
        "\n",
        "news_embedding_df = news_embedding_df.select(\"newsID\", *(col(c) / news_embedding_df.normalization for c in news_embedding_df.columns[1:-1]))\n",
        "old_columns = news_embedding_df.columns[1:]\n",
        "\n",
        "for idx in range(len(old_columns)):\n",
        "  news_embedding_df = news_embedding_df.withColumnRenamed(old_columns[idx], new_columns[idx])\n",
        "\n",
        "\n",
        "# Features vector generated for prediction\n",
        "assembler = VectorAssembler(\n",
        "  inputCols=[str(x) for x in range(1, 101)], outputCol=\"features\"\n",
        ")\n",
        "news_embedding_df = assembler.transform(news_embedding_df).select(\"newsID\", \"features\")\n",
        "\n",
        "# news_embedding_df.show(n=5)\n",
        "# print(news_embedding_df.count())\n",
        "# print(news_embedding_df.select('newsID').dropDuplicates().count())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"newsID NULL value count: \", news_embedding_df.where(col(\"newsID\").isNull()).count())\n",
        "print(\"features NULL value count: \", news_embedding_df.where(col(\"features\").isNull()).count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab4n5THF_lVE",
        "outputId": "b276235e-868d-4032-bc18-25b5f65ba79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newsID NULL value count:  0\n",
            "features NULL value count:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Do content-based recommender system on MINDsmall dataset"
      ],
      "metadata": {
        "id": "o_2lM6_osXKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Prepare the dataset"
      ],
      "metadata": {
        "id": "Ml_-WqBcszHK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sa3ffodJg-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd40de00-52cf-4a8e-8de8-859516767fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----+----------+--------------------+\n",
            "|newsID|userID|click|      time|            features|\n",
            "+------+------+-----+----------+--------------------+\n",
            "| N4530|U63482|  1.0|2019-11-09|[-0.0777836659275...|\n",
            "| N2272|U63482|  1.0|2019-11-09|[-0.1711606694736...|\n",
            "|N36889|U63482|  1.0|2019-11-09|[-0.0296129682490...|\n",
            "|N15198|U63482|  1.0|2019-11-09|[-0.1378691761883...|\n",
            "| N8756|U63482|  0.0|2019-11-09|[-0.0626723382417...|\n",
            "+------+------+-----+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract dataset from one specified user\n",
        "dataset = user_click_df.where(\"userID=='U63482'\")\n",
        "dataset = dataset.join(news_embedding_df, on=[\"newsID\"], how=\"inner\").na.drop(\"any\").orderBy(asc(\"time\"))\n",
        "dataset.show(n=5)\n",
        "# print(dataset.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gBua3QmNZV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20aa6ba7-acd8-458e-ce66-381f32193524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-----+----------+--------------------+\n",
            "|newsID|userID|click|      time|            features|\n",
            "+------+------+-----+----------+--------------------+\n",
            "|N62006|U63482|  1.0|2019-11-09|[0.09825049387673...|\n",
            "|N16209|U63482|  0.0|2019-11-09|[0.04475929334630...|\n",
            "|N21351|U63482|  0.0|2019-11-09|[-0.0177218875516...|\n",
            "|N45331|U63482|  1.0|2019-11-09|[0.07647303509497...|\n",
            "|N15582|U63482|  0.0|2019-11-09|[-0.0467451200857...|\n",
            "+------+------+-----+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+------+------+-----+----------+--------------------+\n",
            "|newsID|userID|click|      time|            features|\n",
            "+------+------+-----+----------+--------------------+\n",
            "| N1539|U63482|  0.0|2019-11-14|[-0.0405669473213...|\n",
            "|N42515|U63482|  0.0|2019-11-14|[0.08227742112814...|\n",
            "|N61829|U63482|  0.0|2019-11-14|[0.24295307281900...|\n",
            "|N34869|U63482|  0.0|2019-11-14|[-0.1178017185116...|\n",
            "|N48613|U63482|  0.0|2019-11-14|[-0.0256017508484...|\n",
            "+------+------+-----+----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into train and test datasets\n",
        "dataset_count = dataset.count()\n",
        "train_count = int(0.9 * dataset_count)\n",
        "test_dataset = dataset\n",
        "train_dataset = test_dataset.limit(train_count)\n",
        "test_dataset = test_dataset.subtract(train_dataset)\n",
        "\n",
        "train_dataset.orderBy(asc(\"time\")).show(n=5)\n",
        "test_dataset.orderBy(asc(\"time\")).show(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Training and testing processes"
      ],
      "metadata": {
        "id": "MswsgbBMt41R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlNvZdh0QaBu"
      },
      "outputs": [],
      "source": [
        "# Prepare data for training and testing processes\n",
        "train_df = train_dataset.select(col(\"click\").alias(\"label\"), \"features\")\n",
        "test_df = test_dataset.select(col(\"click\").alias(\"label\"), \"features\")\n",
        "\n",
        "# Create model and do training process\n",
        "mpl = MultilayerPerceptronClassifier(layers=[100, 50, 20, 5, 2], seed=1)\n",
        "mpl.setMaxIter(100)\n",
        "mpl.setBlockSize(64)\n",
        "model = mpl.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xVLrUpwTvFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbdc826-42ad-4d9b-c6a5-2cf822675048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted precision value on train dataset = 0.6936783586549082\n",
            "Weighted precision value on test dataset = 0.8064386317907446\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "Weighted recall value on train dataset = 0.6912156166814551\n",
            "Weighted recall value on test dataset = 0.6428571428571429\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n",
            "Accuracy value on train dataset = 0.6912156166814551\n",
            "Accuracy value on test dataset = 0.6428571428571429\n",
            "\n",
            "--------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Do validation\n",
        "train_result = model.transform(train_df)\n",
        "train_output_df = train_result.select(\"prediction\", \"label\")\n",
        "\n",
        "test_result = model.transform(test_df)\n",
        "test_output_df = test_result.select(\"prediction\", \"label\")\n",
        "\n",
        "# train_result.show(n=5)\n",
        "# test_result.show(n=5)\n",
        "\n",
        "metrics = ['weightedPrecision', 'weightedRecall', 'accuracy']\n",
        "for metric in metrics:\n",
        "  evaluator = MulticlassClassificationEvaluator(metricName=metric)\n",
        "  if metric == \"weightedPrecision\":\n",
        "    print('Weighted precision value on train dataset = ' + str(evaluator.evaluate(train_output_df)))\n",
        "    print('Weighted precision value on test dataset = ' + str(evaluator.evaluate(test_output_df)))\n",
        "  elif metric == \"weightedRecall\":\n",
        "    print('Weighted recall value on train dataset = ' + str(evaluator.evaluate(train_output_df)))\n",
        "    print('Weighted recall value on test dataset = ' + str(evaluator.evaluate(test_output_df)))\n",
        "  else:\n",
        "    print('Accuracy value on train dataset = ' + str(evaluator.evaluate(train_output_df)))\n",
        "    print('Accuracy value on test dataset = ' + str(evaluator.evaluate(test_output_df)))\n",
        "  print('\\n--------------------------------------------------------\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY2z8BfugCji"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}